{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The paper explores an approach to determine how a patient's family members' medical history influences their disease risk. This is a meaningful problem because it could help inform patients about their risk for certain diseases based on relative and family information. Utlizing family medical history for predicting a patient's disease risk is also complicated by a variety of genetic, environmental, and lifestyle factors.\n",
        "\n",
        "This paper proposes a novel solution to this problem by utilizing a graph-based deep learning approach for learning representations of family member's influence on patient's disease risk. A graph based approach is a more useful and natural way of modeling the connections between family members than previous methods.\n",
        "\n",
        "Previous works have also recognized that it is useful to include information from family members when predicting the risk of disease. However, machine learning approaches using tabular data do not model the underlying geometric structure of family history. Using a graph based approach, this structure is much more easily obtained and modeled.\n",
        "\n",
        "The main contributions of the paper are:\n",
        "  * a scalable, disease-agnostic machine learning tool making use of GNNs and LSTMs which learn representations of a patient's disease risk from family member's medical information.\n",
        "  * Data which shows graph-based approaches perform better than clinically-inspired or deep learning baselines used previously.\n",
        "  * Graph explanability techniques demonstrate that GNN-LSTM embeddings identify medical features which are more suitable for predicting disease risk than features identifies by an epidemiological baseline.\n",
        "\n",
        "The researchers observed that graph-based models consistently outperformed the baseline approaches, although the best performing model between GNN and GNN-LSTM varied depending on the disease in question. Cancers typically performed better on the GNN model, which the researchers believe is due to cancers generally being less hereditary than other diseases."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Whc3jr8gJhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: Graph-based approaches predict disease risk better than the baseline model.\n",
        "2.   Hypothesis 2: The GNN model using GraphConv layers predicts disease risk better than the GNN model using GCN layers.\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the experiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n",
        "Below are the Python libraries used to implement graph representation learning for familial relationships."
      ],
      "metadata": {
        "id": "d-RH5YRYFz15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "T-g_tC3W_7M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import torch\n",
        "import torch_geometric\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Dataset as GraphDataset\n",
        "from torch_geometric.data import Batch\n",
        "import torch_geometric.nn as gnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "from random import choices\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data\n",
        "  * Source of the data: The data is sourced from the Github repository at https://github.com/dsgelab/family-EHR-graphs. Since the paper relies on a nationwide health registry dataset that cannot be publicly shared due to privacty concerns, this dataset is designed to mimic the key properties of the actual data.\n",
        "  * Statistics: This is synthetic data, which means we are able to choose the amount of data we would like to use. In this case, we are using 150000 number of rows. Out of 150000 rows, 57297 are marked as True and 92703 are marked as False\n",
        "  * Data process: We are splitting the data into an 80/20 train/test split. For example, for baseline non-longitudinal data, we are only usig 39,297 rows of data. For training, we are using 31,437 rows of data andn 7860 for testing. As this is synthetic data, there is little to no cleaning required.\n",
        "  * TODO: Print a few rows of synthetic data to discuss the different features.\n",
        "\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_dir = '/content/drive/MyDrive/CS598 DLH/Project/'\n",
        "\n",
        "class DataFetch():\n",
        "    \"\"\"\n",
        "    Class for fetching and formatting data\n",
        "\n",
        "    Expects a tensor list of patients encoded using the numerical node_ids\n",
        "\n",
        "    Assumes maskfile, statfile rows are indexed in order of these node_ids (0, 1, ... num_samples)\n",
        "    and they include data for both the target and graph samples (retrieve data using .iloc)\n",
        "\n",
        "    The edgefile only needs to include data for the target samples, and is indexed\n",
        "    using the node_ids (retrieve data using .loc)\n",
        "\n",
        "    Note that the featfile has exactly one label, corresponding to the label column name in the statfile\n",
        "\n",
        "    Note if the input is a directed graph the code converts it to an undirected graph\n",
        "\n",
        "    Parameters:\n",
        "    maskfile, featfile, statfile and edgefile are filepaths to csv files\n",
        "    sqlpath is the path to the sql database\n",
        "    params is a dictionary of additional parameters (obs_window_start, obs_window_end)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_type, gnn_layer, featfile, alt_featfile=None, local=False):\n",
        "        feat_df = pd.read_csv(raw_data_dir + featfile)\n",
        "        statfile = pd.read_csv(raw_data_dir + 'Gen3_50k_0.7_142857_statfile.csv')\n",
        "\n",
        "        self.local = local\n",
        "        self.static_features = feat_df[feat_df['type']=='static']['name'].tolist()\n",
        "        self.longitudinal_features = feat_df[feat_df['type']=='longitudinal']['name'].tolist()\n",
        "        self.edge_features = feat_df[feat_df['type']=='edge']['name'].tolist()\n",
        "        # some gnn layers only support a single edge weight\n",
        "        if gnn_layer in ['gcn', 'graphconv']: self.edge_features=['weight']\n",
        "        self.label_key = feat_df[feat_df['type']=='label']['name'].tolist()[0]\n",
        "        self.static_data = torch.tensor(statfile[self.static_features].values, dtype=torch.float)\n",
        "        self.label_data = torch.tensor(statfile[self.label_key].to_numpy(), dtype=torch.float32)\n",
        "        # if len(self.longitudinal_features)>0 and not self.local: self.conn = sqlite3.connect(sqlpath)\n",
        "        # self.params = params\n",
        "\n",
        "        if model_type in ['graph', 'graph_no_target', 'explainability']:\n",
        "            # can specify a different feature set using alt_featfile\n",
        "            alt_feat_df = pd.read_csv(raw_data_dir + alt_featfile)\n",
        "            self.alt_static_features = alt_feat_df[alt_feat_df['type']=='static']['name'].tolist()\n",
        "            self.alt_static_data = torch.tensor(statfile[self.alt_static_features].values, dtype=torch.float)\n",
        "\n",
        "        mask_df = pd.read_csv(raw_data_dir + 'Gen3_50k_0.7_142857_maskfile.csv')\n",
        "        self.id_map = dict(zip(mask_df['node_id'], mask_df['PATIENTID']))\n",
        "        self.train_patient_list = torch.tensor(mask_df[mask_df['train']==0]['node_id'].to_numpy())\n",
        "        self.validate_patient_list = torch.tensor(mask_df[mask_df['train']==1]['node_id'].to_numpy())\n",
        "        self.test_patient_list = torch.tensor(mask_df[mask_df['train']==2]['node_id'].to_numpy())\n",
        "        self.num_samples_train_minority_class = torch.sum(self.label_data[self.train_patient_list]==1).item()\n",
        "        self.num_samples_train_majority_class = torch.sum(self.label_data[self.train_patient_list]==0).item()\n",
        "        self.num_samples_valid_minority_class = torch.sum(self.label_data[self.validate_patient_list]==1).item()\n",
        "        self.num_samples_valid_majority_class = torch.sum(self.label_data[self.validate_patient_list]==0).item()\n",
        "\n",
        "        if model_type!='baseline':\n",
        "            self.edge_df = pd.read_csv(raw_data_dir + 'Gen3_50k_0.7_142857_edgefile.csv')\n",
        "            self.edge_df = self.edge_df.groupby('target_patient').agg(list)\n",
        "\n",
        "    def get_static_data(self, patients):\n",
        "        x_static = self.static_data[patients]\n",
        "        y = self.label_data[patients]\n",
        "        return x_static, y\n",
        "\n",
        "    def get_alt_static_data(self, patients):\n",
        "        x_static = self.alt_static_data[patients]\n",
        "        return x_static\n",
        "\n",
        "    def get_longitudinal_data(self, patients):\n",
        "        if self.local:\n",
        "            # return simulated data for local testing\n",
        "            num_nodes = len(patients)\n",
        "            num_years = self.params['obs_window_end']-self.params['obs_window_start']+1\n",
        "            num_features = len(self.longitudinal_features)\n",
        "            num_data = int(num_nodes*num_years*num_features*0.01)\n",
        "            i = [choices(range(num_nodes),k=num_data), choices(range(num_years),k=num_data), choices(range(num_features),k=num_data)]\n",
        "            v = [1]*num_data\n",
        "            x_longitudinal = torch.sparse_coo_tensor(i, v, (num_nodes, num_years, num_features), dtype=torch.float32)\n",
        "            x_longitudinal_dense = x_longitudinal.to_dense()\n",
        "        else:\n",
        "            # fetch data from SQLite database\n",
        "            id_list = [self.id_map[patient.item()] for patient in patients]\n",
        "            data = pd.DataFrame()\n",
        "            for patient in id_list:\n",
        "                command = \"SELECT PATIENTID, EVENT_YEAR, ENDPOINT FROM long WHERE PATIENTID='{}'\".format(patient)\n",
        "                data = pd.concat([data, pd.read_sql_query(command, self.conn)])\n",
        "            data = data[data['ENDPOINT'].isin(self.longitudinal_features)]\n",
        "\n",
        "            # limit to observation window years\n",
        "            data['EVENT_YEAR'] = data['EVENT_YEAR'].astype(int)\n",
        "            data = data[(data['EVENT_YEAR']>=self.params['obs_window_start'])&(data['EVENT_YEAR']<=self.params['obs_window_end'])]\n",
        "\n",
        "            # map to index positions\n",
        "            node_index = dict(zip(id_list, range(len(id_list))))\n",
        "            year_index = dict(zip(np.arange(self.params['obs_window_start'], self.params['obs_window_end']+1), range(self.params['obs_window_end']-self.params['obs_window_start']+1)))\n",
        "            feat_index = dict(zip(self.longitudinal_features, range(len(self.longitudinal_features))))\n",
        "            data['PATIENTID'] = data['PATIENTID'].map(node_index)\n",
        "            data['EVENT_YEAR'] = data['EVENT_YEAR'].map(year_index)\n",
        "            data['ENDPOINT'] = data['ENDPOINT'].map(feat_index)\n",
        "\n",
        "            # create sparse tensor\n",
        "            i = [data['PATIENTID'].tolist(), data['EVENT_YEAR'].tolist(), data['ENDPOINT'].tolist()]\n",
        "            v = [1]*len(data)\n",
        "            x_longitudinal = torch.sparse_coo_tensor(i, v, (len(node_index), len(year_index), len(feat_index)), dtype=torch.float32)\n",
        "            x_longitudinal_dense = x_longitudinal.to_dense()\n",
        "\n",
        "        return x_longitudinal_dense\n",
        "\n",
        "    def get_relatives(self, patients):\n",
        "        \"\"\"Returns a list of node ids included in any of these patient graphs\n",
        "        \"\"\"\n",
        "        return torch.tensor(list(set([i for list in self.edge_df.loc[patients]['node1'].to_list() for i in list] + [i for list in self.edge_df.loc[patients]['node2'].to_list() for i in list])))\n",
        "\n",
        "    def construct_patient_graph(self, patient, all_relatives, all_x_static, all_y, all_x_longitudinal=None):\n",
        "        \"\"\"Creates a re-indexed pytorch geometric data object for the patient\n",
        "        \"\"\"\n",
        "        # order nodes and get indices in all_relatives to retrieve feature data\n",
        "        node_ordering = np.asarray(list(set(self.edge_df.loc[patient].node1 + self.edge_df.loc[patient].node2)))\n",
        "        node_indices = [list(all_relatives.tolist()).index(value) for value in node_ordering]\n",
        "        x_static = all_x_static[node_indices]\n",
        "        y = all_y[list(all_relatives.tolist()).index(patient)] # predicting single value for each graph\n",
        "\n",
        "        # reindex the edge indices from 0, 1, ... num_nodes\n",
        "        node1 = [list(node_ordering.tolist()).index(value) for value in self.edge_df.loc[patient].node1]\n",
        "        node2 = [list(node_ordering.tolist()).index(value) for value in self.edge_df.loc[patient].node2]\n",
        "        edge_index = torch.tensor([node1,node2], dtype=torch.long)\n",
        "        edge_weight = torch.t(torch.tensor(self.edge_df.loc[patient][self.edge_features], dtype=torch.float))\n",
        "\n",
        "        data = torch_geometric.data.Data(x=x_static, edge_index=edge_index, y=y, edge_attr=edge_weight)\n",
        "        transform = torch_geometric.transforms.ToUndirected(reduce='mean')\n",
        "        data = transform(data)\n",
        "        if all_x_longitudinal is not None: data.x_longitudinal = all_x_longitudinal[node_indices]\n",
        "        data.target_index = torch.tensor(list(node_ordering.tolist()).index(patient))\n",
        "        return data\n",
        "\n",
        "\n",
        "class Data(Dataset):\n",
        "    def __init__(self, patient_list, fetch_data):\n",
        "        \"\"\"\n",
        "        Loads non-graph datasets for a given list of patients\n",
        "        Returns (x_static, x_longitudinal, y) if longitudinal data included, else (x_static, y)\n",
        "        \"\"\"\n",
        "        self.patient_list = patient_list\n",
        "        self.num_target_patients = len(patient_list)\n",
        "        self.fetch_data = fetch_data\n",
        "        self.include_longitudinal = len(fetch_data.longitudinal_features)>0\n",
        "\n",
        "    def __getitem__(self, patients):\n",
        "        batch_patient_list = self.patient_list[patients]\n",
        "        x_static, y = self.fetch_data.get_static_data(batch_patient_list)\n",
        "        if self.include_longitudinal:\n",
        "            x_longitudinal = self.fetch_data.get_longitudinal_data(batch_patient_list)\n",
        "            return x_static, x_longitudinal, y\n",
        "        else:\n",
        "            return x_static, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_target_patients\n",
        "\n",
        "\n",
        "class GraphData(GraphDataset):\n",
        "  def __init__(self, patient_list, fetch_data):\n",
        "      \"\"\"\n",
        "      Loads a batch of multiple patient graphs\n",
        "      \"\"\"\n",
        "      self.patient_list = patient_list\n",
        "      self.num_target_patients = len(patient_list)\n",
        "      self.fetch_data = fetch_data\n",
        "      self.include_longitudinal = len(fetch_data.longitudinal_features)>0\n",
        "\n",
        "  def __getitem__(self, patients):\n",
        "      # returns multiple patient graphs by constructing a pytorch geometric Batch object\n",
        "      batch_patient_list = self.patient_list[patients]\n",
        "      data_list = []\n",
        "\n",
        "      # it's more efficient to fetch feature data for all patients and their relatives,\n",
        "      # and then split into separate graphs\n",
        "      all_relatives = self.fetch_data.get_relatives(batch_patient_list)\n",
        "      all_x_static, all_y = self.fetch_data.get_static_data(all_relatives)\n",
        "      patient_x_static = self.fetch_data.get_alt_static_data(batch_patient_list)\n",
        "      if self.include_longitudinal: all_x_longitudinal = self.fetch_data.get_longitudinal_data(all_relatives)\n",
        "      else: all_x_longitudinal = None\n",
        "\n",
        "      patient_index = 0\n",
        "      for patient in batch_patient_list:\n",
        "        patient_graph = self.fetch_data.construct_patient_graph(patient.item(), all_relatives, all_x_static, all_y, all_x_longitudinal)\n",
        "        patient_graph.patient_x_static = patient_x_static[patient_index].reshape(1,-1)\n",
        "        if self.include_longitudinal:\n",
        "            patient_x_longitudinal = patient_graph.x_longitudinal[patient_graph.target_index]\n",
        "            patient_graph.patient_x_longitudinal = patient_x_longitudinal[None,:,:]\n",
        "        data_list.append(patient_graph)\n",
        "        patient_index += 1\n",
        "\n",
        "      batch_data = Batch.from_data_list(data_list)\n",
        "      return batch_data\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.num_target_patients"
      ],
      "metadata": {
        "id": "aT2y5rljzAVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_and_loader(patient_list, fetch_data, model_type, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    patient_list: list of patients (target samples) to load data for\n",
        "    fetch_data: the data object\n",
        "    params: dictionary of other parameters\n",
        "    shuffle: samples in random order if true\n",
        "    \"\"\"\n",
        "    if model_type == 'baseline':\n",
        "        dataset = Data(patient_list, fetch_data)\n",
        "    elif model_type in ['graph', 'graph_no_target', 'explainability']:\n",
        "        dataset = GraphData(patient_list, fetch_data)\n",
        "\n",
        "    if shuffle:\n",
        "        sample_order = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "    else:\n",
        "        sample_order = torch.utils.data.sampler.SequentialSampler(dataset)\n",
        "\n",
        "    sampler = torch.utils.data.sampler.BatchSampler(\n",
        "        sample_order,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=False)\n",
        "\n",
        "    loader = DataLoader(dataset, sampler=sampler, num_workers=1)\n",
        "    return dataset, loader\n"
      ],
      "metadata": {
        "id": "RnWXo-CfMCfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##   Model\n",
        "The model includes the model definition which usually is a class, model training, and other necessary parts.\n",
        "\n",
        "* Training Objectives: We are using the Adam optimizer and the WeightedBCELoss loss function. Our dropout rate is 0.5 and our learning rate is 0.001.\n",
        "\n",
        "  ### Baseline Model\n",
        "  * Model architecture: The Baseline Model has three linear layers, with a ReLU activation function, Sigmoid activation function, and a Dropout layer.\n",
        "  * The model is not pretrained, and the training code is shown below.\n",
        "\n",
        "  ### GNN Model\n",
        "  * Model Architecture: The GNN model can be customized more than the Baseline model. There are two GNN layers in use in this model, which can be one of three types: GCN, GraphConv, or GAT. Additionally, there can be one of many types of pooling methods: target, sum, mean, etc. The model also has quite a few linear layers that is chooses between based on for different parts of the data. One final Linear layer is used to retrieve patient results, and a different final Linear layer is used to retrieve family results.\n",
        "  * The model is not pretrained, and the training code is shown below."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline(torch.nn.Module):\n",
        "    def __init__(self, num_features_static, hidden_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.static_linear1 = nn.Linear(num_features_static, hidden_dim)\n",
        "        self.static_linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.final_linear = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x_static):\n",
        "        linear_out = self.relu(self.static_linear1(x_static))\n",
        "        linear_out = self.relu(self.static_linear2(linear_out))\n",
        "        linear_out = self.dropout(linear_out)\n",
        "        out = self.sigmoid(self.final_linear(linear_out))\n",
        "        return out\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, num_features_static_graph, num_features_static_node, hidden_dim, gnn_layer, pooling_method, dropout_rate, ratio):\n",
        "        super().__init__()\n",
        "        self.pooling_method = pooling_method\n",
        "        self.static_linear1 = nn.Linear(num_features_static_node, hidden_dim)\n",
        "        self.static_linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # which gnn layer to use is specified by input argument\n",
        "        if gnn_layer=='gcn':\n",
        "            print(\"Using GCN layers\")\n",
        "            self.conv1 = gnn.GCNConv(num_features_static_graph, hidden_dim)\n",
        "            self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
        "        if gnn_layer=='graphconv':\n",
        "            print(\"Using GraphConv layers\")\n",
        "            self.conv1 = gnn.GraphConv(num_features_static_graph, hidden_dim)\n",
        "            self.conv2 = gnn.GraphConv(hidden_dim, hidden_dim)\n",
        "        elif gnn_layer=='gat':\n",
        "            print(\"Using GAT layers\")\n",
        "            self.conv1 = gnn.GATConv(num_features_static_graph, hidden_dim)\n",
        "            self.conv2 = gnn.GATConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.pre_final_linear = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.final_linear_com = nn.Linear(hidden_dim, 1)\n",
        "        self.final_linear = nn.Linear(hidden_dim, 1)\n",
        "        self.final_linear1 = nn.Linear(hidden_dim, 1)\n",
        "        self.final_linear2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.TopKpool = gnn.TopKPooling(hidden_dim, ratio=ratio)\n",
        "        self.SAGpool = gnn.SAGPooling(hidden_dim, ratio=ratio)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index):\n",
        "        # patient part of the network\n",
        "        linear_out = self.relu(self.static_linear1(x_static_node))\n",
        "        linear_out = self.relu(self.static_linear2(linear_out))\n",
        "        patient_out = self.dropout(linear_out)\n",
        "\n",
        "        # family part of the network\n",
        "        gnn_out = self.relu(self.conv1(x_static_graph, edge_index, edge_weight))\n",
        "        gnn_out = self.relu(self.conv2(gnn_out, edge_index, edge_weight))\n",
        "\n",
        "        if self.pooling_method=='target':\n",
        "            out = gnn_out[target_index] # instead of pooling, use the target node embedding\n",
        "        elif self.pooling_method=='sum':\n",
        "            out = gnn.global_add_pool(gnn_out, batch)\n",
        "        elif self.pooling_method=='mean':\n",
        "            out = gnn.global_mean_pool(gnn_out, batch)\n",
        "        elif self.pooling_method=='topkpool_sum':\n",
        "            out, pool_edge_index, pool_edge_attr, pool_batch, _, _ = self.TopKpool(gnn_out, edge_index, edge_weight, batch)\n",
        "            out = gnn.global_add_pool(out, pool_batch)\n",
        "        elif self.pooling_method=='topkpool_mean':\n",
        "            out, pool_edge_index, pool_edge_attr, pool_batch, _, _ = self.TopKpool(gnn_out, edge_index, edge_weight, batch)\n",
        "            out = gnn.global_mean_pool(out, pool_batch)\n",
        "        elif self.pooling_method=='sagpool_sum':\n",
        "            out, pool_edge_index, pool_edge_attr, pool_batch, _, _ = self.SAGpool(gnn_out, edge_index, edge_weight, batch)\n",
        "            out = gnn.global_add_pool(out, pool_batch)\n",
        "        elif self.pooling_method=='sagpool_mean':\n",
        "            out, pool_edge_index, pool_edge_attr, pool_batch, _, _ = self.SAGpool(gnn_out, edge_index, edge_weight, batch)\n",
        "            out = gnn.global_mean_pool(out, pool_batch)\n",
        "\n",
        "        family_out = self.dropout(out)\n",
        "\n",
        "        # combined part of network (classifiation output)\n",
        "        out = torch.cat((patient_out, family_out), 1)\n",
        "        out = self.relu(self.pre_final_linear(out))\n",
        "        out = self.sigmoid(self.final_linear_com(out))\n",
        "        # separate output heads for different parts of the network (for loss calculations)\n",
        "        patient_out = self.sigmoid(self.final_linear1(patient_out))\n",
        "        family_out = self.sigmoid(self.final_linear2(family_out))\n",
        "        return out, patient_out, family_out"
      ],
      "metadata": {
        "id": "5x4tzdo4Cmqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Loss function\n",
        "Class imbalance arises because the models are predicting diseases with prevalence < 10% in the selected cohorts. The authors of the paper alleviate this by using class-weighted loss functions, sampling strategies designed for imbalanced classification, methods to prevent overfitting, and careful choice and interpretation of evaluation metrics (Wharrie, Sophie, et al.).\n"
      ],
      "metadata": {
        "id": "_U7BIR7nCyWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedBCELoss(torch.nn.Module):\n",
        "    def __init__(self, num_samples_dataset, num_samples_minority_class, num_samples_majority_class):\n",
        "        super(WeightedBCELoss,self).__init__()\n",
        "        self.num_samples_dataset = num_samples_dataset\n",
        "        self.num_samples_minority_class = num_samples_minority_class\n",
        "        self.num_samples_majority_class = num_samples_majority_class\n",
        "\n",
        "    def forward(self, y_est, y):\n",
        "        weight_minority = self.num_samples_dataset / self.num_samples_minority_class\n",
        "        weight_majority = self.num_samples_dataset / self.num_samples_majority_class\n",
        "        class_weights = torch.tensor([[weight_minority] if i==1 else [weight_majority] for i in y])\n",
        "        bce_loss = torch.nn.BCELoss(weight=class_weights)\n",
        "        weighted_bce_loss = bce_loss(y_est, y)\n",
        "        return weighted_bce_loss\n"
      ],
      "metadata": {
        "id": "BEH4x0h12who"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline A2 - Age, Sex, and family history MLP**"
      ],
      "metadata": {
        "id": "2ad0QsP89UjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type='baseline'\n",
        "fetch_data = DataFetch(model_type=model_type, featfile='featfiles/featfile_A2.csv', gnn_layer='graphconv')\n",
        "train_patient_list = fetch_data.train_patient_list\n",
        "validate_patient_list = fetch_data.validate_patient_list\n",
        "num_features_static = len(fetch_data.static_features)\n",
        "num_samples_train_dataset = len(train_patient_list)\n",
        "num_samples_valid_dataset = len(validate_patient_list)\n",
        "num_samples_train_minority_class = fetch_data.num_samples_train_minority_class\n",
        "num_samples_valid_minority_class = fetch_data.num_samples_valid_minority_class\n",
        "num_samples_train_majority_class = fetch_data.num_samples_train_majority_class\n",
        "num_samples_valid_majority_class = fetch_data.num_samples_valid_majority_class\n",
        "print('static_data: ', len(fetch_data.static_data))\n",
        "print('label data: ', len(fetch_data.label_data))\n",
        "print('True label: ', fetch_data.label_data.sum().item())\n",
        "print('False label: ', len(fetch_data.label_data) -fetch_data.label_data.sum().item())\n",
        "print('Train dataset: ', len(train_patient_list))\n",
        "print('Validate dataset: ', len(validate_patient_list))\n",
        "print('Test dataset: ', len(fetch_data.test_patient_list))"
      ],
      "metadata": {
        "id": "-zJoziGVFrth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=250\n",
        "main_hidden_dim = 20\n",
        "lstm_hidden_dim = 20\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.001\n",
        "num_batches_train = int(np.ceil(len(train_patient_list)/batch_size))\n",
        "num_batches_validate = int(np.ceil(len(validate_patient_list)/batch_size))\n",
        "\n",
        "model = Baseline(num_features_static, main_hidden_dim, dropout_rate)\n",
        "loss_func = WeightedBCELoss(num_samples_train_dataset, num_samples_train_minority_class, num_samples_train_majority_class)\n",
        "valid_loss_func = WeightedBCELoss(num_samples_valid_dataset, num_samples_valid_minority_class, num_samples_valid_majority_class)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_dataset, train_loader = get_data_and_loader(train_patient_list, fetch_data, model_type, batch_size)\n",
        "validate_dataset, validate_loader = get_data_and_loader(validate_patient_list, fetch_data, model_type, batch_size)\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "for i in range(num_epoch):\n",
        "  model.train()\n",
        "  epoch_train_loss = []\n",
        "  for train_batch in tqdm(train_loader, total=num_batches_train):\n",
        "    x_static, y = train_batch[0][0], train_batch[1][0].unsqueeze(1)\n",
        "    output = model(x_static)\n",
        "    model_output = {'output': output}\n",
        "    loss = loss_func(output, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_train_loss.append(loss.item())\n",
        "\n",
        "  # eval on validset\n",
        "  model.eval()\n",
        "  epoch_valid_loss = []\n",
        "  valid_output = np.array([])\n",
        "  valid_y = np.array([])\n",
        "  for valid_batch in tqdm(validate_loader, total=num_batches_validate):\n",
        "    x_static, y = valid_batch[0][0], valid_batch[1][0].unsqueeze(1)\n",
        "    output = model(x_static)\n",
        "    valid_output = np.concatenate((valid_output, output.reshape(-1).detach().cpu().numpy()))\n",
        "    valid_y = np.concatenate((valid_y, y.reshape(-1).detach().cpu().numpy()))\n",
        "    loss = valid_loss_func(output, y)\n",
        "    epoch_valid_loss.append(loss.item())\n",
        "\n",
        "  train_loss, valid_loss = np.mean(epoch_train_loss), np.mean(epoch_valid_loss)\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(valid_y, valid_output)\n",
        "gmeans = np.sqrt(tpr * (1-fpr))\n",
        "ix = np.argmax(gmeans)\n",
        "threshold = thresholds[ix]"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n",
        "\n",
        "# Results\n",
        "TODO: Test both models and create graphs of different metrics to compare.<br>\n",
        "Baseline:<br>\n",
        "* Area Under the Receiver Operating Characteristic Curve (auc_roc): 0.759335902988548\n",
        "* Compute Area Under the Curve (auc): 0.2731897339160587<br>\n",
        "* F1 score: 0.32642619595804556<br>\n",
        "* Recall: 0.7308132875143184<br>\n",
        "* MCC: 0.2501420694207095\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def brier_skill_score(actual_y, predicted_prob_y):\n",
        "    e = sum(actual_y) / len(actual_y)\n",
        "    bs_ref = sum((e-actual_y)**2) / len(actual_y)\n",
        "    bs = sum((predicted_prob_y-actual_y)**2) / len(actual_y)\n",
        "    bss = 1 - bs / bs_ref\n",
        "    return bss\n",
        "\n",
        "def calculate_metrics(actual_y, predicted_y, predicted_prob_y):\n",
        "    auc_roc = metrics.roc_auc_score(actual_y, predicted_prob_y)\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(actual_y, predicted_prob_y)\n",
        "    auc_prc = metrics.auc(recall, precision)\n",
        "    mcc = metrics.matthews_corrcoef(actual_y, predicted_y)\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(actual_y, predicted_y).ravel()\n",
        "    ts = tp / (tp + fn + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = (2*tp) / (2*tp + fp + fn)\n",
        "    bss = brier_skill_score(actual_y, predicted_prob_y)\n",
        "\n",
        "    metric_results = {'metric_auc_roc':auc_roc, # typically reported, but can be biased for imbalanced classes\n",
        "               'metric_auc_prc':auc_prc, # better suited for imbalanced classes\n",
        "               'metric_f1':f1, # also should be better suited for imbalanced classes\n",
        "               'metric_recall':recall, # important for medical studies, to reduce misses of positive instances\n",
        "               'metric_mcc':mcc, # correlation that is suitable for imbalanced classes\n",
        "               'metric_ts':ts, # suited for rare events, penalizing misclassification as the rare event (fp)\n",
        "               'metric_bss':bss, # brier skill score, where higher score corresponds better calibration of predicted probabilities\n",
        "               'true_negatives':tn,\n",
        "               'false_positives':fp,\n",
        "               'false_negatives':fn,\n",
        "               'true_positives':tp}\n",
        "\n",
        "    return metric_results"
      ],
      "metadata": {
        "id": "vc9g8ymgN_6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "num_samples = 3\n",
        "test_output = [np.array([]) for _ in range(num_samples)]\n",
        "test_y = [np.array([]) for _ in range(num_samples)]\n",
        "representations = pd.DataFrame()\n",
        "\n",
        "test_patient_list = fetch_data.test_patient_list\n",
        "num_batches_test = int(np.ceil(len(test_patient_list)/batch_size))\n",
        "test_dataset, test_loader = get_data_and_loader(test_patient_list, fetch_data, model_type, batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "for m in model.modules():\n",
        "  if m.__class__.__name__.startswith('Dropout'):\n",
        "    m.train()\n",
        "\n",
        "for sample in range(num_samples):\n",
        "    for test_batch in tqdm(test_loader, total=num_batches_test):\n",
        "        x_static, y = test_batch[0][0], test_batch[1][0].unsqueeze(1)\n",
        "        output = model(x_static)\n",
        "        test_output[sample] = np.concatenate((test_output[sample], output.reshape(-1).detach().cpu().numpy()))\n",
        "        test_y[sample] = np.concatenate((test_y[sample], y.reshape(-1).detach().cpu().numpy()))\n",
        "\n",
        "# report standard error for uncertainty\n",
        "test_output_se = np.array(test_output).std(axis=0) / np.sqrt(num_samples)\n",
        "\n",
        "# take average over all samples to get expected value\n",
        "test_output = np.array(test_output).mean(axis=0)\n",
        "test_y = np.array(test_y).mean(axis=0)\n",
        "\n",
        "results = pd.DataFrame({'actual':test_y, 'pred_raw':test_output, 'pred_raw_se':test_output_se})\n",
        "results['pred_binary'] = (results['pred_raw']>threshold).astype(int)\n",
        "metric_results = calculate_metrics(results['actual'], results['pred_binary'], results['pred_raw'])\n",
        "print(metric_results)\n",
        "\n",
        "# plot figures to better show the results\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(valid_losses, label='Validate')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphConv Graph model\n",
        "The GNN model using GraphConv layers for non-longitudinal data."
      ],
      "metadata": {
        "id": "cwYsM340QVD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare data variables, loss function, and optimizer\n",
        "model_type = 'graph'\n",
        "pooling_method = 'target'\n",
        "ratio = 0.5\n",
        "gamma = 1\n",
        "alpha = 1\n",
        "beta = 1\n",
        "delta = 1\n",
        "fetch_data = DataFetch(model_type=model_type, featfile='featfiles/featfile_A2.csv', gnn_layer='graphconv', alt_featfile='featfiles/featfile_G1.csv')\n",
        "train_patient_list = fetch_data.train_patient_list\n",
        "validate_patient_list = fetch_data.validate_patient_list\n",
        "num_features_static = len(fetch_data.static_features)\n",
        "num_features_alt_static = len(fetch_data.alt_static_features)\n",
        "num_samples_train_dataset = len(train_patient_list)\n",
        "num_samples_valid_dataset = len(validate_patient_list)\n",
        "num_samples_train_minority_class = fetch_data.num_samples_train_minority_class\n",
        "num_samples_valid_minority_class = fetch_data.num_samples_valid_minority_class\n",
        "num_samples_train_majority_class = fetch_data.num_samples_train_majority_class\n",
        "num_samples_valid_majority_class = fetch_data.num_samples_valid_majority_class\n",
        "model = GNN(num_features_static, num_features_alt_static, main_hidden_dim, 'graphconv', pooling_method, dropout_rate, ratio)\n",
        "\n",
        "loss_func = WeightedBCELoss(num_samples_train_dataset, num_samples_train_minority_class, num_samples_train_majority_class)\n",
        "valid_loss_func = WeightedBCELoss(num_samples_valid_dataset, num_samples_valid_minority_class, num_samples_valid_majority_class)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset, train_loader = get_data_and_loader(train_patient_list, fetch_data, model_type, batch_size)\n",
        "validate_dataset, validate_loader = get_data_and_loader(validate_patient_list, fetch_data, model_type, batch_size)\n",
        "print('static_data: ', len(fetch_data.static_data))\n",
        "print('label data: ', len(fetch_data.label_data))\n",
        "print('True label: ', fetch_data.label_data.sum().item())\n",
        "print('False label: ', len(fetch_data.label_data) -fetch_data.label_data.sum().item())\n",
        "print('Train dataset: ', len(train_patient_list))\n",
        "print('Validate dataset: ', len(validate_patient_list))\n",
        "print('Test dataset: ', len(fetch_data.test_patient_list))"
      ],
      "metadata": {
        "id": "ZjOAiT9lQOJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "separate_loss_terms = {'NN_train':[], 'target_train':[], 'family_train':[], 'lstm_train':[], 'NN_valid':[], 'target_valid':[], 'family_valid':[], 'lstm_valid':[]}\n",
        "for i in range(num_epoch):\n",
        "  model.train()\n",
        "  epoch_train_loss = []\n",
        "  separate_loss_terms_epoch = {'NN_train':[], 'target_train':[], 'family_train':[], 'lstm_train':[], 'NN_valid':[], 'target_valid':[], 'family_valid':[], 'lstm_valid':[]}\n",
        "\n",
        "  for train_batch in tqdm(train_loader, total=num_batches_train):\n",
        "    x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "    output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "    # combined loss that considers the additive effect of patient and family effects\n",
        "    loss_term_NN = gamma * loss_func(output, y)\n",
        "    loss_term_target = alpha * loss_func(patient_output, y)\n",
        "    loss_term_family = beta * loss_func(family_output, y)\n",
        "    separate_loss_terms_epoch['NN_train'].append(loss_term_NN.item())\n",
        "    separate_loss_terms_epoch['target_train'].append(loss_term_target.item())\n",
        "    separate_loss_terms_epoch['family_train'].append(loss_term_family.item())\n",
        "    loss = loss_term_NN + loss_term_target + loss_term_family\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_train_loss.append(loss.item())\n",
        "\n",
        "  # eval on validset\n",
        "  model.eval()\n",
        "  epoch_valid_loss = []\n",
        "  valid_output = np.array([])\n",
        "  valid_y = np.array([])\n",
        "  for valid_batch in tqdm(validate_loader, total=num_batches_validate):\n",
        "    x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "    output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "\n",
        "    valid_output = np.concatenate((valid_output, output.reshape(-1).detach().cpu().numpy()))\n",
        "    valid_y = np.concatenate((valid_y, y.reshape(-1).detach().cpu().numpy()))\n",
        "\n",
        "    # combined loss that considers the additive effect of patient and family effects\n",
        "    loss_term_NN = gamma * valid_loss_func(output, y)\n",
        "    loss_term_target = alpha * valid_loss_func(patient_output, y)\n",
        "    loss_term_family = beta * valid_loss_func(family_output, y)\n",
        "    separate_loss_terms_epoch['NN_train'].append(loss_term_NN.item())\n",
        "    separate_loss_terms_epoch['target_train'].append(loss_term_target.item())\n",
        "    separate_loss_terms_epoch['family_train'].append(loss_term_family.item())\n",
        "    loss = loss_term_NN + loss_term_target + loss_term_family\n",
        "    epoch_valid_loss.append(loss.item())\n",
        "\n",
        "  train_loss, valid_loss = np.mean(epoch_train_loss), np.mean(epoch_valid_loss)\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(valid_y, valid_output)\n",
        "gmeans = np.sqrt(tpr * (1-fpr))\n",
        "ix = np.argmax(gmeans)\n",
        "threshold = thresholds[ix]\n",
        "\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "num_samples = 3\n",
        "test_output = [np.array([]) for _ in range(num_samples)]\n",
        "test_y = [np.array([]) for _ in range(num_samples)]\n",
        "representations = pd.DataFrame()\n",
        "\n",
        "test_patient_list = fetch_data.test_patient_list\n",
        "num_batches_test = int(np.ceil(len(test_patient_list)/batch_size))\n",
        "test_dataset, test_loader = get_data_and_loader(test_patient_list, fetch_data, model_type, batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "for m in model.modules():\n",
        "  if m.__class__.__name__.startswith('Dropout'):\n",
        "    m.train()\n",
        "\n",
        "for sample in range(num_samples):\n",
        "    for test_batch in tqdm(test_loader, total=num_batches_test):\n",
        "        x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "        output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "        test_output[sample] = np.concatenate((test_output[sample], output.reshape(-1).detach().cpu().numpy()))\n",
        "        test_y[sample] = np.concatenate((test_y[sample], y.reshape(-1).detach().cpu().numpy()))\n",
        "\n",
        "# report standard error for uncertainty\n",
        "test_output_se = np.array(test_output).std(axis=0) / np.sqrt(num_samples)\n",
        "\n",
        "# take average over all samples to get expected value\n",
        "test_output = np.array(test_output).mean(axis=0)\n",
        "test_y = np.array(test_y).mean(axis=0)\n",
        "\n",
        "results = pd.DataFrame({'actual':test_y, 'pred_raw':test_output, 'pred_raw_se':test_output_se})\n",
        "results['pred_binary'] = (results['pred_raw']>threshold).astype(int)\n",
        "metric_results = calculate_metrics(results['actual'], results['pred_binary'], results['pred_raw'])\n",
        "print(metric_results)\n",
        "\n",
        "# plot figures to better show the results\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(valid_losses, label='Validate')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "k0--dp7KQ0Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN Graph Model\n",
        "\n",
        "The GNN model using GCN layers for non-logitudinal data."
      ],
      "metadata": {
        "id": "IgT53tpJ8Uc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare data variables, loss function, and optimizer\n",
        "model_type = 'graph'\n",
        "pooling_method = 'target'\n",
        "ratio = 0.5\n",
        "gamma = 1\n",
        "alpha = 1\n",
        "beta = 1\n",
        "delta = 1\n",
        "fetch_data = DataFetch(model_type=model_type, featfile='featfiles/featfile_A2.csv', gnn_layer='graphconv', alt_featfile='featfiles/featfile_G1.csv')\n",
        "train_patient_list = fetch_data.train_patient_list\n",
        "validate_patient_list = fetch_data.validate_patient_list\n",
        "num_features_static = len(fetch_data.static_features)\n",
        "num_features_alt_static = len(fetch_data.alt_static_features)\n",
        "num_samples_train_dataset = len(train_patient_list)\n",
        "num_samples_valid_dataset = len(validate_patient_list)\n",
        "num_samples_train_minority_class = fetch_data.num_samples_train_minority_class\n",
        "num_samples_valid_minority_class = fetch_data.num_samples_valid_minority_class\n",
        "num_samples_train_majority_class = fetch_data.num_samples_train_majority_class\n",
        "num_samples_valid_majority_class = fetch_data.num_samples_valid_majority_class\n",
        "model = GNN(num_features_static, num_features_alt_static, main_hidden_dim, 'gcn', pooling_method, dropout_rate, ratio)\n",
        "\n",
        "loss_func = WeightedBCELoss(num_samples_train_dataset, num_samples_train_minority_class, num_samples_train_majority_class)\n",
        "valid_loss_func = WeightedBCELoss(num_samples_valid_dataset, num_samples_valid_minority_class, num_samples_valid_majority_class)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset, train_loader = get_data_and_loader(train_patient_list, fetch_data, model_type, batch_size)\n",
        "validate_dataset, validate_loader = get_data_and_loader(validate_patient_list, fetch_data, model_type, batch_size)"
      ],
      "metadata": {
        "id": "SqG_-vD18TNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "separate_loss_terms = {'NN_train':[], 'target_train':[], 'family_train':[], 'lstm_train':[], 'NN_valid':[], 'target_valid':[], 'family_valid':[], 'lstm_valid':[]}\n",
        "for i in range(num_epoch):\n",
        "  model.train()\n",
        "  epoch_train_loss = []\n",
        "  separate_loss_terms_epoch = {'NN_train':[], 'target_train':[], 'family_train':[], 'lstm_train':[], 'NN_valid':[], 'target_valid':[], 'family_valid':[], 'lstm_valid':[]}\n",
        "\n",
        "  for train_batch in tqdm(train_loader, total=num_batches_train):\n",
        "    x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "    output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "    # combined loss that considers the additive effect of patient and family effects\n",
        "    loss_term_NN = gamma * loss_func(output, y)\n",
        "    loss_term_target = alpha * loss_func(patient_output, y)\n",
        "    loss_term_family = beta * loss_func(family_output, y)\n",
        "    separate_loss_terms_epoch['NN_train'].append(loss_term_NN.item())\n",
        "    separate_loss_terms_epoch['target_train'].append(loss_term_target.item())\n",
        "    separate_loss_terms_epoch['family_train'].append(loss_term_family.item())\n",
        "    loss = loss_term_NN + loss_term_target + loss_term_family\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_train_loss.append(loss.item())\n",
        "\n",
        "  # eval on validset\n",
        "  model.eval()\n",
        "  epoch_valid_loss = []\n",
        "  valid_output = np.array([])\n",
        "  valid_y = np.array([])\n",
        "  for valid_batch in tqdm(validate_loader, total=num_batches_validate):\n",
        "    x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "    output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "\n",
        "    valid_output = np.concatenate((valid_output, output.reshape(-1).detach().cpu().numpy()))\n",
        "    valid_y = np.concatenate((valid_y, y.reshape(-1).detach().cpu().numpy()))\n",
        "\n",
        "    # combined loss that considers the additive effect of patient and family effects\n",
        "    loss_term_NN = gamma * valid_loss_func(output, y)\n",
        "    loss_term_target = alpha * valid_loss_func(patient_output, y)\n",
        "    loss_term_family = beta * valid_loss_func(family_output, y)\n",
        "    separate_loss_terms_epoch['NN_train'].append(loss_term_NN.item())\n",
        "    separate_loss_terms_epoch['target_train'].append(loss_term_target.item())\n",
        "    separate_loss_terms_epoch['family_train'].append(loss_term_family.item())\n",
        "    loss = loss_term_NN + loss_term_target + loss_term_family\n",
        "    epoch_valid_loss.append(loss.item())\n",
        "\n",
        "  train_loss, valid_loss = np.mean(epoch_train_loss), np.mean(epoch_valid_loss)\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(valid_y, valid_output)\n",
        "gmeans = np.sqrt(tpr * (1-fpr))\n",
        "ix = np.argmax(gmeans)\n",
        "threshold = thresholds[ix]\n",
        "\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "rVrPIUFO8niF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "num_samples = 3\n",
        "test_output = [np.array([]) for _ in range(num_samples)]\n",
        "test_y = [np.array([]) for _ in range(num_samples)]\n",
        "representations = pd.DataFrame()\n",
        "\n",
        "test_patient_list = fetch_data.test_patient_list\n",
        "num_batches_test = int(np.ceil(len(test_patient_list)/batch_size))\n",
        "test_dataset, test_loader = get_data_and_loader(test_patient_list, fetch_data, model_type, batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "for m in model.modules():\n",
        "  if m.__class__.__name__.startswith('Dropout'):\n",
        "    m.train()\n",
        "\n",
        "for sample in range(num_samples):\n",
        "    for test_batch in tqdm(test_loader, total=num_batches_test):\n",
        "        x_static_node, x_static_graph, y, edge_index, edge_weight, batch, target_index = train_batch.patient_x_static, train_batch.x, train_batch.y.unsqueeze(1), train_batch.edge_index, train_batch.edge_attr, train_batch.batch, train_batch.target_index\n",
        "        output, patient_output, family_output = model(x_static_node, x_static_graph, edge_index, edge_weight, batch, target_index)\n",
        "        test_output[sample] = np.concatenate((test_output[sample], output.reshape(-1).detach().cpu().numpy()))\n",
        "        test_y[sample] = np.concatenate((test_y[sample], y.reshape(-1).detach().cpu().numpy()))\n",
        "\n",
        "# report standard error for uncertainty\n",
        "test_output_se = np.array(test_output).std(axis=0) / np.sqrt(num_samples)\n",
        "\n",
        "# take average over all samples to get expected value\n",
        "test_output = np.array(test_output).mean(axis=0)\n",
        "test_y = np.array(test_y).mean(axis=0)\n",
        "\n",
        "results = pd.DataFrame({'actual':test_y, 'pred_raw':test_output, 'pred_raw_se':test_output_se})\n",
        "results['pred_binary'] = (results['pred_raw']>threshold).astype(int)\n",
        "metric_results = calculate_metrics(results['actual'], results['pred_binary'], results['pred_raw'])\n",
        "print(metric_results)\n",
        "\n",
        "# plot figures to better show the results\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(valid_losses, label='Validate')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "alhdrQ0LERRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n",
        "# Discussion\n",
        "\n",
        "This paper is reproducible, and by following along with the paper itself, the related GitHub repository, and the synthetic data generator, we were able to replicate the paper's results. The authors made great efforts in making their work easily reproducible. Providing a synthetic data generator greatly simplified the process of replicating their work. Additionally, they provided a table comparing the results of their models to the baseline models, which made it simple to see if our work matched theirs.\n",
        "\n",
        "TODO:\n",
        "We also saw that the <GCN/GraphConv> graph model performed better than the <GCN/GraphConv> model. This <aligns/does not align> with the hypothese we made above.\n",
        "\n",
        "In the next phase of the project, we will implement more of the ablations mentioned in the paper. For example, we would like to test various data features in the models. It would also be interesting to investigate different structures for the GNN model to see if we can improve the accuracy of the model."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}